SEED: 12345 # Random seed for reproducibility of sampled datasets and model initialisation
MODEL: 'EN0' # EN:[0,2,S,M,L,X] (e.g. ENS): CN:[P,N,T,S,B,L] (e.g. CNB); VT:[T,S,B,L] (e.g. VTS) or pretrained filename with model as first 3 char e.g. '/model/ens_pt.keras'
SAVEFILE: 'case_study' # Filename to save .keras model : MODEL name appended automatically
OUTPUT_PATH: '/data/output' # Save Path for output files (model, class map, confusion matrix): SAVEFILE/MODEL added automatically to path

# Hugging Face Model to initialise weights: replace with custom model if desired or use "NA" in FILENAME to default to ImageNet weights
# IMPORTANT: Ensure that the model specified below in FILENAME is compatible with the model architecture specified in MODEL!
REPO_ID: "bwbrook/mewc_pretrained" # Repository ID to be used for subsequent fine-tuning on training data, e.g. "bwbrook/mewc_pretrained"
FILENAME: "NA" # HF filename to source base model, e.g. "ens_mewc_case_study.keras" or "NA" for no pretrained model

TRAIN_PATH: '/data/train' # Path to training data (subfolders for each class)
TEST_PATH: '/data/test' # Path to hold-out test data (for final model evaluation)
VAL_PATH: '/data/test' # Path to validation data (optional, for hyperparameter tuning, otherwise set to same as TEST_PATH)

OPTIMIZER: 'adamw' # Gradient-based optimisation algorithm. Options: 'adamw' (default: adaptive), 'rmsprop' (simple) or 'lion' (sign-based)
OPTIM_REG: 1e-4 # Regularisation (weight decay) parameter for optimiser. Default: 1e-4, 0 = off, typical range: 1e-6 (weak) to 1e-2 (strong)
LR_SCHEDULER: 'expon' # Learning-rate scheduler: 'expon' (monotonic decline), 'cosine' (cyclic, warm restarts), 'polynomial' (cyclic)
LEARNING_RATE: 1e-4 # Recommended default initial LR: 1e-4, typical range: 1e-5 (slower, steady) to 1e-2 (faster, oscillatory convergence)
BATCH_SIZE: 16 # Mini-batch size (depends on GPU memory, if memory or kernel error is generated then reduce accordingly, typically 4 to 128)
NUM_AUG: 3 # Number of per-image random augmentation layers to be applied to the training samples (default: 3, suggested range: 1-5)

FROZ_EPOCH: 10 # Number of epochs to train the frozen model: usually ~10 required to converge the dense classifier prior to fine-tuning

BUF: 1 # Blocks to unfreeze for fine-tuning (suggest 1-2 for EN/CN, 5-9 for VT). Set to 0 to keep base frozen or -1 to train on full model
PROG_STAGE_LEN: 7  # Number of progressive fine-tuning epochs (see sequence of Magnitudes and Dropouts augmentations below) prior to final stage
PROG_TOT_EPOCH: 30 # Total number of epochs required will depend on size of CLASS_SAMPLES_[DEFAULT|SPECIFIC] (larger requires fewer epochs per stage)

# Regularisation/augmentation lists define the sequence of progressive training stages and must be of same length: default 4 but can add or delete  
MAGNITUDES: # RandAug magnitudes (range 0-1), increased in stages, details: https://keras.io/api/keras_cv/layers/augmentation/rand_augment/
  - 0.2
  - 0.4
  - 0.6
  - 0.8
DROPOUTS: # Stochastic dropout (0-1 range), increased in stages: drops random proportion of connections for regularisation during training
  - 0.1
  - 0.2
  - 0.3
  - 0.4

# Class samples default value and specific values for training data
CLASS_SAMPLES_DEFAULT: 4000

# Use list of dicts for specificity to allow for mapping sample counts to classes
CLASS_SAMPLES_SPECIFIC:
#   - SAMPLES: 3000
#     CLASS: "13"
#   - SAMPLES: 3000
#     CLASS: "11"
#   - SAMPLES: 2000
#     CLASS: "3"
#   - SAMPLES: 1000
#     CLASS: "1"
#   - SAMPLES: 1000
#     CLASS: "2"
#   - SAMPLES: 1000
#     CLASS: "7"
